{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7pKE+5VP9Bams8pQia9uy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lokeshgadhi/introonpandas/blob/main/EDA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 1. Data Exploration and Preprocessing:\n",
        "# •\tLoad the dataset and conduct basic data exploration (summary statistics, missing values, data types).\n",
        "# •\tHandle missing values as per the best practices (imputation, removal, etc.).\n",
        "# •\tApply scaling techniques to numerical features:\n",
        "# •\tStandard Scaling\n",
        "# •\tMin-Max Scaling\n",
        "# •\tDiscuss the scenarios where each scaling technique is preferred and why.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Load the dataset\n",
        "# Replace 'your_dataset.csv' with the actual path to your dataset\n",
        "try:\n",
        "    df = pd.read_csv('adult_with_headers.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'adult_with_headers.csv' not found. Please replace it with the correct file path.\")\n",
        "    # Create a dummy DataFrame for demonstration if the file is not found\n",
        "    data = {'col1': [1, 2, 3, 4, 5, None],\n",
        "            'col2': [10, 20, None, 40, 50, 60],\n",
        "            'col3': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
        "            'col4': [100, 200, 300, 400, 500, 600]}\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"Using a dummy DataFrame for demonstration.\")\n",
        "\n",
        "\n",
        "# Basic data exploration\n",
        "print(\"--- Basic Data Exploration ---\")\n",
        "print(\"Head of the dataset:\")\n",
        "print(df.head())\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(df.describe())\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Handle missing values\n",
        "# Example: Impute missing numerical values with the mean\n",
        "for col in df.select_dtypes(include=['number']).columns:\n",
        "    if df[col].isnull().any():\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "        print(f\"Imputed missing values in '{col}' with the mean ({mean_value:.2f})\")\n",
        "\n",
        "# Example: Remove rows with missing values in categorical columns (if any remain)\n",
        "# Identify columns that are not numerical and still have missing values after numerical imputation\n",
        "categorical_cols_with_missing = df.select_dtypes(exclude=['number']).columns[df.select_dtypes(exclude=['number']).isnull().any()]\n",
        "if not categorical_cols_with_missing.empty:\n",
        "    initial_rows = len(df)\n",
        "    df.dropna(subset=categorical_cols_with_missing, inplace=True)\n",
        "    rows_removed = initial_rows - len(df)\n",
        "    if rows_removed > 0:\n",
        "        print(f\"\\nRemoved {rows_removed} rows with missing values in categorical columns: {list(categorical_cols_with_missing)}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- After Handling Missing Values ---\")\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Apply scaling techniques to numerical features\n",
        "# Select numerical columns for scaling\n",
        "numerical_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "if not numerical_cols.empty:\n",
        "    print(\"\\n--- Applying Scaling Techniques ---\")\n",
        "\n",
        "    # Standard Scaling\n",
        "    print(\"\\nApplying Standard Scaling...\")\n",
        "    scaler_standard = StandardScaler()\n",
        "    df_standard_scaled = df.copy()\n",
        "    df_standard_scaled[numerical_cols] = scaler_standard.fit_transform(df[numerical_cols])\n",
        "    print(\"Standard Scaled DataFrame Head:\")\n",
        "    print(df_standard_scaled[numerical_cols].head())\n",
        "    print(\"Standard Scaled DataFrame Describe:\")\n",
        "    print(df_standard_scaled[numerical_cols].describe())\n",
        "\n",
        "    # Min-Max Scaling\n",
        "    print(\"\\nApplying Min-Max Scaling...\")\n",
        "    scaler_minmax = MinMaxScaler()\n",
        "    df_minmax_scaled = df.copy()\n",
        "    df_minmax_scaled[numerical_cols] = scaler_minmax.fit_transform(df[numerical_cols])\n",
        "    print(\"Min-Max Scaled DataFrame Head:\")\n",
        "    print(df_minmax_scaled[numerical_cols].head())\n",
        "    print(\"Min-Max Scaled DataFrame Describe:\")\n",
        "    print(df_minmax_scaled[numerical_cols].describe())\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo numerical columns found for scaling.\")\n",
        "\n",
        "print(\"\\n--- Discussion on Scaling Techniques ---\")\n",
        "print(\"Standard Scaling:\")\n",
        "print(\"  - Centers the data around the mean with a unit standard deviation.\")\n",
        "print(\"  - Formula: z = (x - μ) / σ\")\n",
        "print(\"  - Scenarios: Preferred when the feature distribution is approximately Gaussian or when the algorithm (e.g., Linear Regression, Logistic Regression, Support Vector Machines with RBF kernel, PCA, K-Means) assumes zero mean and unit variance. It is less affected by outliers compared to Min-Max scaling because it uses the standard deviation in the denominator.\")\n",
        "\n",
        "print(\"\\nMin-Max Scaling:\")\n",
        "print(\"  - Scales the data to a fixed range, usually between 0 and 1.\")\n",
        "print(\"  - Formula: z = (x - min) / (max - min)\")\n",
        "print(\"  - Scenarios: Preferred when the feature range is more important than the distribution shape, or when the algorithm (e.g., Neural Networks) is sensitive to the scale of features but not necessarily their distribution. It is more affected by outliers, as extreme values will influence the max and min used in the scaling.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_mw9ZiBJr9TC",
        "outputId": "6bc4f39b-09f7-4418-bba9-f4932d8e4286"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Basic Data Exploration ---\n",
            "Head of the dataset:\n",
            "   age          workclass  fnlwgt   education  education_num  \\\n",
            "0   39          State-gov   77516   Bachelors             13   \n",
            "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
            "2   38            Private  215646     HS-grad              9   \n",
            "3   53            Private  234721        11th              7   \n",
            "4   28            Private  338409   Bachelors             13   \n",
            "\n",
            "        marital_status          occupation    relationship    race      sex  \\\n",
            "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
            "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
            "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
            "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
            "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
            "\n",
            "   capital_gain  capital_loss  hours_per_week  native_country  income  \n",
            "0          2174             0              40   United-States   <=50K  \n",
            "1             0             0              13   United-States   <=50K  \n",
            "2             0             0              40   United-States   <=50K  \n",
            "3             0             0              40   United-States   <=50K  \n",
            "4             0             0              40            Cuba   <=50K  \n",
            "\n",
            "Summary statistics:\n",
            "                age        fnlwgt  education_num  capital_gain  capital_loss  \\\n",
            "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
            "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
            "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
            "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
            "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
            "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
            "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
            "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
            "\n",
            "       hours_per_week  \n",
            "count    32561.000000  \n",
            "mean        40.437456  \n",
            "std         12.347429  \n",
            "min          1.000000  \n",
            "25%         40.000000  \n",
            "50%         40.000000  \n",
            "75%         45.000000  \n",
            "max         99.000000  \n",
            "\n",
            "Missing values per column:\n",
            "age               0\n",
            "workclass         0\n",
            "fnlwgt            0\n",
            "education         0\n",
            "education_num     0\n",
            "marital_status    0\n",
            "occupation        0\n",
            "relationship      0\n",
            "race              0\n",
            "sex               0\n",
            "capital_gain      0\n",
            "capital_loss      0\n",
            "hours_per_week    0\n",
            "native_country    0\n",
            "income            0\n",
            "dtype: int64\n",
            "\n",
            "Data types:\n",
            "age                int64\n",
            "workclass         object\n",
            "fnlwgt             int64\n",
            "education         object\n",
            "education_num      int64\n",
            "marital_status    object\n",
            "occupation        object\n",
            "relationship      object\n",
            "race              object\n",
            "sex               object\n",
            "capital_gain       int64\n",
            "capital_loss       int64\n",
            "hours_per_week     int64\n",
            "native_country    object\n",
            "income            object\n",
            "dtype: object\n",
            "\n",
            "--- After Handling Missing Values ---\n",
            "Missing values per column:\n",
            "age               0\n",
            "workclass         0\n",
            "fnlwgt            0\n",
            "education         0\n",
            "education_num     0\n",
            "marital_status    0\n",
            "occupation        0\n",
            "relationship      0\n",
            "race              0\n",
            "sex               0\n",
            "capital_gain      0\n",
            "capital_loss      0\n",
            "hours_per_week    0\n",
            "native_country    0\n",
            "income            0\n",
            "dtype: int64\n",
            "\n",
            "--- Applying Scaling Techniques ---\n",
            "\n",
            "Applying Standard Scaling...\n",
            "Standard Scaled DataFrame Head:\n",
            "        age    fnlwgt  education_num  capital_gain  capital_loss  \\\n",
            "0  0.030671 -1.063611       1.134739      0.148453      -0.21666   \n",
            "1  0.837109 -1.008707       1.134739     -0.145920      -0.21666   \n",
            "2 -0.042642  0.245079      -0.420060     -0.145920      -0.21666   \n",
            "3  1.057047  0.425801      -1.197459     -0.145920      -0.21666   \n",
            "4 -0.775768  1.408176       1.134739     -0.145920      -0.21666   \n",
            "\n",
            "   hours_per_week  \n",
            "0       -0.035429  \n",
            "1       -2.222153  \n",
            "2       -0.035429  \n",
            "3       -0.035429  \n",
            "4       -0.035429  \n",
            "Standard Scaled DataFrame Describe:\n",
            "                age        fnlwgt  education_num  capital_gain  capital_loss  \\\n",
            "count  3.256100e+04  3.256100e+04   3.256100e+04  3.256100e+04  3.256100e+04   \n",
            "mean  -2.705915e-17 -1.001625e-16   1.471887e-16  1.309314e-17  1.016900e-16   \n",
            "std    1.000015e+00  1.000015e+00   1.000015e+00  1.000015e+00  1.000015e+00   \n",
            "min   -1.582206e+00 -1.681631e+00  -3.529656e+00 -1.459205e-01 -2.166595e-01   \n",
            "25%   -7.757679e-01 -6.816910e-01  -4.200596e-01 -1.459205e-01 -2.166595e-01   \n",
            "50%   -1.159546e-01 -1.082193e-01  -3.136003e-02 -1.459205e-01 -2.166595e-01   \n",
            "75%    6.904838e-01  4.478765e-01   7.460392e-01 -1.459205e-01 -2.166595e-01   \n",
            "max    3.769612e+00  1.226856e+01   2.300838e+00  1.339458e+01  1.059351e+01   \n",
            "\n",
            "       hours_per_week  \n",
            "count    3.256100e+04  \n",
            "mean    -1.549355e-17  \n",
            "std      1.000015e+00  \n",
            "min     -3.194030e+00  \n",
            "25%     -3.542945e-02  \n",
            "50%     -3.542945e-02  \n",
            "75%      3.695194e-01  \n",
            "max      4.742967e+00  \n",
            "\n",
            "Applying Min-Max Scaling...\n",
            "Min-Max Scaled DataFrame Head:\n",
            "        age    fnlwgt  education_num  capital_gain  capital_loss  \\\n",
            "0  0.301370  0.044302       0.800000       0.02174           0.0   \n",
            "1  0.452055  0.048238       0.800000       0.00000           0.0   \n",
            "2  0.287671  0.138113       0.533333       0.00000           0.0   \n",
            "3  0.493151  0.151068       0.400000       0.00000           0.0   \n",
            "4  0.150685  0.221488       0.800000       0.00000           0.0   \n",
            "\n",
            "   hours_per_week  \n",
            "0        0.397959  \n",
            "1        0.122449  \n",
            "2        0.397959  \n",
            "3        0.397959  \n",
            "4        0.397959  \n",
            "Min-Max Scaled DataFrame Describe:\n",
            "                age        fnlwgt  education_num  capital_gain  capital_loss  \\\n",
            "count  32561.000000  32561.000000   32561.000000  32561.000000  32561.000000   \n",
            "mean       0.295639      0.120545       0.605379      0.010777      0.020042   \n",
            "std        0.186855      0.071685       0.171515      0.073854      0.092507   \n",
            "min        0.000000      0.000000       0.000000      0.000000      0.000000   \n",
            "25%        0.150685      0.071679       0.533333      0.000000      0.000000   \n",
            "50%        0.273973      0.112788       0.600000      0.000000      0.000000   \n",
            "75%        0.424658      0.152651       0.733333      0.000000      0.000000   \n",
            "max        1.000000      1.000000       1.000000      1.000000      1.000000   \n",
            "\n",
            "       hours_per_week  \n",
            "count    32561.000000  \n",
            "mean         0.402423  \n",
            "std          0.125994  \n",
            "min          0.000000  \n",
            "25%          0.397959  \n",
            "50%          0.397959  \n",
            "75%          0.448980  \n",
            "max          1.000000  \n",
            "\n",
            "--- Discussion on Scaling Techniques ---\n",
            "Standard Scaling:\n",
            "  - Centers the data around the mean with a unit standard deviation.\n",
            "  - Formula: z = (x - μ) / σ\n",
            "  - Scenarios: Preferred when the feature distribution is approximately Gaussian or when the algorithm (e.g., Linear Regression, Logistic Regression, Support Vector Machines with RBF kernel, PCA, K-Means) assumes zero mean and unit variance. It is less affected by outliers compared to Min-Max scaling because it uses the standard deviation in the denominator.\n",
            "\n",
            "Min-Max Scaling:\n",
            "  - Scales the data to a fixed range, usually between 0 and 1.\n",
            "  - Formula: z = (x - min) / (max - min)\n",
            "  - Scenarios: Preferred when the feature range is more important than the distribution shape, or when the algorithm (e.g., Neural Networks) is sensitive to the scale of features but not necessarily their distribution. It is more affected by outliers, as extreme values will influence the max and min used in the scaling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 2. Encoding Techniques:\n",
        "# •\tApply One-Hot Encoding to categorical variables with less than 5 categories.\n",
        "# •\tUse Label Encoding for categorical variables with more than 5 categories.\n",
        "# •\tDiscuss the pros and cons of One-Hot Encoding and Label Encoding.\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "print(\"\\n--- Applying Encoding Techniques ---\")\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "print(f\"\\nCategorical columns identified: {list(categorical_cols)}\")\n",
        "\n",
        "# Separate columns for One-Hot and Label Encoding\n",
        "onehot_cols = [col for col in categorical_cols if df[col].nunique() < 5]\n",
        "label_cols = [col for col in categorical_cols if df[col].nunique() >= 5]\n",
        "\n",
        "print(f\"\\nColumns for One-Hot Encoding (less than 5 categories): {onehot_cols}\")\n",
        "print(f\"Columns for Label Encoding (5 or more categories): {label_cols}\")\n",
        "\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "if onehot_cols:\n",
        "    print(\"\\nApplying One-Hot Encoding...\")\n",
        "    # Initialize OneHotEncoder. handle_unknown='ignore' can be useful for unseen categories in test sets.\n",
        "    # sparse_output=False makes the output a numpy array instead of a sparse matrix.\n",
        "    onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "    # Fit and transform the selected columns\n",
        "    onehot_encoded_data = onehot_encoder.fit_transform(df[onehot_cols])\n",
        "\n",
        "    # Create a DataFrame from the one-hot encoded data with appropriate column names\n",
        "    # Get feature names after fitting\n",
        "    onehot_feature_names = onehot_encoder.get_feature_names_out(onehot_cols)\n",
        "    df_onehot_encoded = pd.DataFrame(onehot_encoded_data, columns=onehot_feature_names, index=df.index)\n",
        "\n",
        "    # Drop the original one-hot encoded columns and concatenate the new ones\n",
        "    df = df.drop(columns=onehot_cols)\n",
        "    df = pd.concat([df, df_onehot_encoded], axis=1)\n",
        "\n",
        "    print(\"DataFrame head after One-Hot Encoding:\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"\\nNo columns found for One-Hot Encoding based on the category threshold.\")\n",
        "\n",
        "\n",
        "# Apply Label Encoding\n",
        "if label_cols:\n",
        "    print(\"\\nApplying Label Encoding...\")\n",
        "    # Initialize LabelEncoder\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    for col in label_cols:\n",
        "        # Apply Label Encoding to each selected column\n",
        "        df[col] = label_encoder.fit_transform(df[col])\n",
        "        print(f\"Applied Label Encoding to '{col}'\")\n",
        "\n",
        "    print(\"DataFrame head after Label Encoding:\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"\\nNo columns found for Label Encoding based on the category threshold.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Discussion on Encoding Techniques ---\")\n",
        "print(\"One-Hot Encoding:\")\n",
        "print(\"  - Creates a new binary column for each unique category in the original feature.\")\n",
        "print(\"  - Each row will have a 1 in the column corresponding to its category and 0s elsewhere.\")\n",
        "print(\"  - Pros:\")\n",
        "print(\"    - Avoids imposing an artificial ordinal relationship between categories.\")\n",
        "print(\"    - Suitable for nominal categorical data (categories without a natural order).\")\n",
        "print(\"    - Many machine learning algorithms (e.g., linear models, SVMs) expect numerical input and work well with one-hot encoded data.\")\n",
        "print(\"  - Cons:\")\n",
        "print(\"    - Can lead to a high-dimensional dataset if a categorical variable has many unique categories (curse of dimensionality).\")\n",
        "print(\"    - Can increase multicollinearity if the categories are not mutually exclusive.\")\n",
        "\n",
        "print(\"\\nLabel Encoding:\")\n",
        "print(\"  - Assigns a unique integer to each unique category in the original feature.\")\n",
        "print(\"  - Pros:\")\n",
        "print(\"    - Simple and space-efficient.\")\n",
        "print(\"    - Preserves the dimensionality of the dataset.\")\n",
        "print(\"  - Cons:\")\n",
        "print(\"    - Imposes an artificial ordinal relationship between categories based on the assigned integers.\")\n",
        "print(\"    - This can be problematic for algorithms that interpret these integers as having a meaningful order or magnitude (e.g., linear models, tree-based models might sometimes be less affected but can still be influenced).\")\n",
        "print(\"    - Best suited for ordinal categorical data (categories with a natural order, which is often not the case for nominal features). Using it for nominal data can lead to misleading results.\")\n",
        "\n",
        "print(\"\\nChoosing between One-Hot and Label Encoding depends on:\")\n",
        "print(\"  - The nature of the categorical variable (nominal vs. ordinal).\")\n",
        "print(\"  - The machine learning algorithm you are using.\")\n",
        "print(\"  - The number of unique categories in the feature.\")\n",
        "print(\"  - Memory and computational constraints.\")\n",
        "\n",
        "print(\"\\nFinal DataFrame head after all preprocessing steps:\")\n",
        "print(df.head())\n",
        "print(\"\\nFinal DataFrame info:\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IpzGHH7usR_I",
        "outputId": "573f6516-17cc-4e9d-8227-a82c528896df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Applying Encoding Techniques ---\n",
            "\n",
            "Categorical columns identified: ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']\n",
            "\n",
            "Columns for One-Hot Encoding (less than 5 categories): ['sex', 'income']\n",
            "Columns for Label Encoding (5 or more categories): ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'native_country']\n",
            "\n",
            "Applying One-Hot Encoding...\n",
            "DataFrame head after One-Hot Encoding:\n",
            "   age          workclass  fnlwgt   education  education_num  \\\n",
            "0   39          State-gov   77516   Bachelors             13   \n",
            "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
            "2   38            Private  215646     HS-grad              9   \n",
            "3   53            Private  234721        11th              7   \n",
            "4   28            Private  338409   Bachelors             13   \n",
            "\n",
            "        marital_status          occupation    relationship    race  \\\n",
            "0        Never-married        Adm-clerical   Not-in-family   White   \n",
            "1   Married-civ-spouse     Exec-managerial         Husband   White   \n",
            "2             Divorced   Handlers-cleaners   Not-in-family   White   \n",
            "3   Married-civ-spouse   Handlers-cleaners         Husband   Black   \n",
            "4   Married-civ-spouse      Prof-specialty            Wife   Black   \n",
            "\n",
            "   capital_gain  capital_loss  hours_per_week  native_country  sex_ Female  \\\n",
            "0          2174             0              40   United-States          0.0   \n",
            "1             0             0              13   United-States          0.0   \n",
            "2             0             0              40   United-States          0.0   \n",
            "3             0             0              40   United-States          0.0   \n",
            "4             0             0              40            Cuba          1.0   \n",
            "\n",
            "   sex_ Male  income_ <=50K  income_ >50K  \n",
            "0        1.0            1.0           0.0  \n",
            "1        1.0            1.0           0.0  \n",
            "2        1.0            1.0           0.0  \n",
            "3        1.0            1.0           0.0  \n",
            "4        0.0            1.0           0.0  \n",
            "\n",
            "Applying Label Encoding...\n",
            "Applied Label Encoding to 'workclass'\n",
            "Applied Label Encoding to 'education'\n",
            "Applied Label Encoding to 'marital_status'\n",
            "Applied Label Encoding to 'occupation'\n",
            "Applied Label Encoding to 'relationship'\n",
            "Applied Label Encoding to 'race'\n",
            "Applied Label Encoding to 'native_country'\n",
            "DataFrame head after Label Encoding:\n",
            "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
            "0   39          7   77516          9             13               4   \n",
            "1   50          6   83311          9             13               2   \n",
            "2   38          4  215646         11              9               0   \n",
            "3   53          4  234721          1              7               2   \n",
            "4   28          4  338409          9             13               2   \n",
            "\n",
            "   occupation  relationship  race  capital_gain  capital_loss  hours_per_week  \\\n",
            "0           1             1     4          2174             0              40   \n",
            "1           4             0     4             0             0              13   \n",
            "2           6             1     4             0             0              40   \n",
            "3           6             0     2             0             0              40   \n",
            "4          10             5     2             0             0              40   \n",
            "\n",
            "   native_country  sex_ Female  sex_ Male  income_ <=50K  income_ >50K  \n",
            "0              39          0.0        1.0            1.0           0.0  \n",
            "1              39          0.0        1.0            1.0           0.0  \n",
            "2              39          0.0        1.0            1.0           0.0  \n",
            "3              39          0.0        1.0            1.0           0.0  \n",
            "4               5          1.0        0.0            1.0           0.0  \n",
            "\n",
            "--- Discussion on Encoding Techniques ---\n",
            "One-Hot Encoding:\n",
            "  - Creates a new binary column for each unique category in the original feature.\n",
            "  - Each row will have a 1 in the column corresponding to its category and 0s elsewhere.\n",
            "  - Pros:\n",
            "    - Avoids imposing an artificial ordinal relationship between categories.\n",
            "    - Suitable for nominal categorical data (categories without a natural order).\n",
            "    - Many machine learning algorithms (e.g., linear models, SVMs) expect numerical input and work well with one-hot encoded data.\n",
            "  - Cons:\n",
            "    - Can lead to a high-dimensional dataset if a categorical variable has many unique categories (curse of dimensionality).\n",
            "    - Can increase multicollinearity if the categories are not mutually exclusive.\n",
            "\n",
            "Label Encoding:\n",
            "  - Assigns a unique integer to each unique category in the original feature.\n",
            "  - Pros:\n",
            "    - Simple and space-efficient.\n",
            "    - Preserves the dimensionality of the dataset.\n",
            "  - Cons:\n",
            "    - Imposes an artificial ordinal relationship between categories based on the assigned integers.\n",
            "    - This can be problematic for algorithms that interpret these integers as having a meaningful order or magnitude (e.g., linear models, tree-based models might sometimes be less affected but can still be influenced).\n",
            "    - Best suited for ordinal categorical data (categories with a natural order, which is often not the case for nominal features). Using it for nominal data can lead to misleading results.\n",
            "\n",
            "Choosing between One-Hot and Label Encoding depends on:\n",
            "  - The nature of the categorical variable (nominal vs. ordinal).\n",
            "  - The machine learning algorithm you are using.\n",
            "  - The number of unique categories in the feature.\n",
            "  - Memory and computational constraints.\n",
            "\n",
            "Final DataFrame head after all preprocessing steps:\n",
            "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
            "0   39          7   77516          9             13               4   \n",
            "1   50          6   83311          9             13               2   \n",
            "2   38          4  215646         11              9               0   \n",
            "3   53          4  234721          1              7               2   \n",
            "4   28          4  338409          9             13               2   \n",
            "\n",
            "   occupation  relationship  race  capital_gain  capital_loss  hours_per_week  \\\n",
            "0           1             1     4          2174             0              40   \n",
            "1           4             0     4             0             0              13   \n",
            "2           6             1     4             0             0              40   \n",
            "3           6             0     2             0             0              40   \n",
            "4          10             5     2             0             0              40   \n",
            "\n",
            "   native_country  sex_ Female  sex_ Male  income_ <=50K  income_ >50K  \n",
            "0              39          0.0        1.0            1.0           0.0  \n",
            "1              39          0.0        1.0            1.0           0.0  \n",
            "2              39          0.0        1.0            1.0           0.0  \n",
            "3              39          0.0        1.0            1.0           0.0  \n",
            "4               5          1.0        0.0            1.0           0.0  \n",
            "\n",
            "Final DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32561 entries, 0 to 32560\n",
            "Data columns (total 17 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   age             32561 non-null  int64  \n",
            " 1   workclass       32561 non-null  int64  \n",
            " 2   fnlwgt          32561 non-null  int64  \n",
            " 3   education       32561 non-null  int64  \n",
            " 4   education_num   32561 non-null  int64  \n",
            " 5   marital_status  32561 non-null  int64  \n",
            " 6   occupation      32561 non-null  int64  \n",
            " 7   relationship    32561 non-null  int64  \n",
            " 8   race            32561 non-null  int64  \n",
            " 9   capital_gain    32561 non-null  int64  \n",
            " 10  capital_loss    32561 non-null  int64  \n",
            " 11  hours_per_week  32561 non-null  int64  \n",
            " 12  native_country  32561 non-null  int64  \n",
            " 13  sex_ Female     32561 non-null  float64\n",
            " 14  sex_ Male       32561 non-null  float64\n",
            " 15  income_ <=50K   32561 non-null  float64\n",
            " 16  income_ >50K    32561 non-null  float64\n",
            "dtypes: float64(4), int64(13)\n",
            "memory usage: 4.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 3. Feature Engineering:\n",
        "# •\tCreate at least 2 new features that could be beneficial for the model. Explain the rationale behind your choices.\n",
        "# •\tApply a transformation (e.g., log transformation) to at least one skewed numerical feature and justify your choice.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 3. Feature Engineering\n",
        "\n",
        "print(\"\\n--- Feature Engineering ---\")\n",
        "\n",
        "# Create new features\n",
        "\n",
        "# Feature 1: 'hours_per_week_age_ratio'\n",
        "# Rationale: This feature might capture how an individual's work intensity (hours per week) relates to their age.\n",
        "# Older individuals might work fewer hours, or conversely, older individuals still working long hours might be distinct.\n",
        "# This could be a non-linear interaction effect that linear models don't capture well on their own.\n",
        "if 'hours-per-week' in df.columns and 'age' in df.columns:\n",
        "    df['hours_per_week_age_ratio'] = df['hours-per-week'] / (df['age'] + 1e-6) # Add small constant to avoid division by zero\n",
        "    print(\"\\nCreated feature: 'hours_per_week_age_ratio'\")\n",
        "else:\n",
        "    print(\"\\nCould not create 'hours_per_week_age_ratio': required columns ('hours-per-week', 'age') not found.\")\n",
        "\n",
        "# Feature 2: 'income_per_year' (assuming 'capital-gain' and 'capital-loss' represent annual values)\n",
        "# Rationale: This attempts to aggregate different forms of capital income into a single feature.\n",
        "# The net capital gain/loss combined with other income features could provide a more complete picture of an individual's financial situation.\n",
        "# Note: This is a simplification and assumes these are annual figures.\n",
        "if 'capital-gain' in df.columns and 'capital-loss' in df.columns:\n",
        "    df['capital_net'] = df['capital-gain'] - df['capital-loss']\n",
        "    print(\"Created feature: 'capital_net'\")\n",
        "else:\n",
        "     print(\"Could not create 'capital_net': required columns ('capital-gain', 'capital-loss') not found.\")\n",
        "\n",
        "print(\"\\nDataFrame head with new features:\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Apply transformation to a skewed numerical feature\n",
        "\n",
        "# Identify numerical columns (after potential new feature creation)\n",
        "numerical_cols_after_fe = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "# Check for skewness\n",
        "print(\"\\nChecking skewness of numerical features:\")\n",
        "skewness = df[numerical_cols_after_fe].skew().sort_values(ascending=False)\n",
        "print(skewness)\n",
        "\n",
        "# Select a highly skewed feature for transformation (e.g., 'capital-gain' or 'capital_net' are often skewed)\n",
        "# Let's choose 'capital-gain' if it exists and is skewed. If not, pick the most skewed one among the candidates.\n",
        "feature_to_transform = None\n",
        "if 'capital-gain' in skewness.index and abs(skewness['capital-gain']) > 1: # Threshold for high skewness\n",
        "    feature_to_transform = 'capital-gain'\n",
        "elif 'capital_net' in skewness.index and abs(skewness['capital_net']) > 1:\n",
        "     feature_to_transform = 'capital_net'\n",
        "else:\n",
        "    # Find the most skewed numerical column\n",
        "    most_skewed_col = skewness.index[0] if not skewness.empty else None\n",
        "    if most_skewed_col is not None and abs(skewness[most_skewed_col]) > 1:\n",
        "         feature_to_transform = most_skewed_col\n",
        "    else:\n",
        "        print(\"\\nNo highly skewed numerical features found for transformation.\")\n",
        "\n",
        "if feature_to_transform:\n",
        "    print(f\"\\nSelected '{feature_to_transform}' for transformation based on skewness ({skewness[feature_to_transform]:.2f}).\")\n",
        "\n",
        "    # Visualize the original distribution\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df[feature_to_transform], kde=True)\n",
        "    plt.title(f'Original Distribution of {feature_to_transform}')\n",
        "\n",
        "    # Apply log transformation (add 1 to handle zeros)\n",
        "    # Log transformation is suitable for positively skewed data.\n",
        "    # log1p (log(1+x)) is often used to handle features that contain zero values.\n",
        "    df[f'{feature_to_transform}_log'] = np.log1p(df[feature_to_transform])\n",
        "    print(f\"Applied log1p transformation to '{feature_to_transform}', created '{feature_to_transform}_log'\")\n",
        "\n",
        "    # Visualize the transformed distribution\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.histplot(df[f'{feature_to_transform}_log'], kde=True)\n",
        "    plt.title(f'Log1p Transformed Distribution of {feature_to_transform}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nRationale for Log Transformation:\")\n",
        "    print(f\"The feature '{feature_to_transform}' shows significant positive skewness, with a long tail towards higher values.\")\n",
        "    print(\"Many machine learning algorithms (especially linear models) assume that features are normally distributed or at least symmetrically distributed.\")\n",
        "    print(\"Highly skewed features can disproportionately influence these models.\")\n",
        "    print(\"Log transformation (log1p) is a common technique used to reduce the skewness of positively skewed data.\")\n",
        "    print(\"It compresses the range of values and can help make the distribution more symmetric, bringing it closer to a normal distribution.\")\n",
        "    print(\"This can lead to better model performance and interpretability.\")\n",
        "    print(\"We use log1p (log(1+x)) instead of log(x) to handle instances where the feature value is 0.\")\n",
        "\n",
        "print(\"\\nDataFrame head after potential transformation:\")\n",
        "print(df.head())\n",
        "print(\"\\nFinal DataFrame info after Feature Engineering:\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2283
        },
        "id": "g0UBmoSKscux",
        "outputId": "7ac2a810-9352-4c68-d51f-822a947625bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Feature Engineering ---\n",
            "\n",
            "Could not create 'hours_per_week_age_ratio': required columns ('hours-per-week', 'age') not found.\n",
            "Could not create 'capital_net': required columns ('capital-gain', 'capital-loss') not found.\n",
            "\n",
            "DataFrame head with new features:\n",
            "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
            "0   39          7   77516          9             13               4   \n",
            "1   50          6   83311          9             13               2   \n",
            "2   38          4  215646         11              9               0   \n",
            "3   53          4  234721          1              7               2   \n",
            "4   28          4  338409          9             13               2   \n",
            "\n",
            "   occupation  relationship  race  capital_gain  capital_loss  hours_per_week  \\\n",
            "0           1             1     4          2174             0              40   \n",
            "1           4             0     4             0             0              13   \n",
            "2           6             1     4             0             0              40   \n",
            "3           6             0     2             0             0              40   \n",
            "4          10             5     2             0             0              40   \n",
            "\n",
            "   native_country  sex_ Female  sex_ Male  income_ <=50K  income_ >50K  \n",
            "0              39          0.0        1.0            1.0           0.0  \n",
            "1              39          0.0        1.0            1.0           0.0  \n",
            "2              39          0.0        1.0            1.0           0.0  \n",
            "3              39          0.0        1.0            1.0           0.0  \n",
            "4               5          1.0        0.0            1.0           0.0  \n",
            "\n",
            "Checking skewness of numerical features:\n",
            "capital_gain      11.953848\n",
            "capital_loss       4.594629\n",
            "fnlwgt             1.446980\n",
            "income_ >50K       1.212430\n",
            "relationship       0.786818\n",
            "sex_ Female        0.719293\n",
            "age                0.558743\n",
            "hours_per_week     0.227643\n",
            "occupation         0.114583\n",
            "marital_status    -0.013508\n",
            "education_num     -0.311676\n",
            "sex_ Male         -0.719293\n",
            "workclass         -0.752024\n",
            "education         -0.934042\n",
            "income_ <=50K     -1.212430\n",
            "race              -2.435386\n",
            "native_country    -3.658303\n",
            "dtype: float64\n",
            "\n",
            "Selected 'capital_gain' for transformation based on skewness (11.95).\n",
            "Applied log1p transformation to 'capital_gain', created 'capital_gain_log'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rationale for Log Transformation:\n",
            "The feature 'capital_gain' shows significant positive skewness, with a long tail towards higher values.\n",
            "Many machine learning algorithms (especially linear models) assume that features are normally distributed or at least symmetrically distributed.\n",
            "Highly skewed features can disproportionately influence these models.\n",
            "Log transformation (log1p) is a common technique used to reduce the skewness of positively skewed data.\n",
            "It compresses the range of values and can help make the distribution more symmetric, bringing it closer to a normal distribution.\n",
            "This can lead to better model performance and interpretability.\n",
            "We use log1p (log(1+x)) instead of log(x) to handle instances where the feature value is 0.\n",
            "\n",
            "DataFrame head after potential transformation:\n",
            "   age  workclass  fnlwgt  education  education_num  marital_status  \\\n",
            "0   39          7   77516          9             13               4   \n",
            "1   50          6   83311          9             13               2   \n",
            "2   38          4  215646         11              9               0   \n",
            "3   53          4  234721          1              7               2   \n",
            "4   28          4  338409          9             13               2   \n",
            "\n",
            "   occupation  relationship  race  capital_gain  capital_loss  hours_per_week  \\\n",
            "0           1             1     4          2174             0              40   \n",
            "1           4             0     4             0             0              13   \n",
            "2           6             1     4             0             0              40   \n",
            "3           6             0     2             0             0              40   \n",
            "4          10             5     2             0             0              40   \n",
            "\n",
            "   native_country  sex_ Female  sex_ Male  income_ <=50K  income_ >50K  \\\n",
            "0              39          0.0        1.0            1.0           0.0   \n",
            "1              39          0.0        1.0            1.0           0.0   \n",
            "2              39          0.0        1.0            1.0           0.0   \n",
            "3              39          0.0        1.0            1.0           0.0   \n",
            "4               5          1.0        0.0            1.0           0.0   \n",
            "\n",
            "   capital_gain_log  \n",
            "0          7.684784  \n",
            "1          0.000000  \n",
            "2          0.000000  \n",
            "3          0.000000  \n",
            "4          0.000000  \n",
            "\n",
            "Final DataFrame info after Feature Engineering:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32561 entries, 0 to 32560\n",
            "Data columns (total 18 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   age               32561 non-null  int64  \n",
            " 1   workclass         32561 non-null  int64  \n",
            " 2   fnlwgt            32561 non-null  int64  \n",
            " 3   education         32561 non-null  int64  \n",
            " 4   education_num     32561 non-null  int64  \n",
            " 5   marital_status    32561 non-null  int64  \n",
            " 6   occupation        32561 non-null  int64  \n",
            " 7   relationship      32561 non-null  int64  \n",
            " 8   race              32561 non-null  int64  \n",
            " 9   capital_gain      32561 non-null  int64  \n",
            " 10  capital_loss      32561 non-null  int64  \n",
            " 11  hours_per_week    32561 non-null  int64  \n",
            " 12  native_country    32561 non-null  int64  \n",
            " 13  sex_ Female       32561 non-null  float64\n",
            " 14  sex_ Male         32561 non-null  float64\n",
            " 15  income_ <=50K     32561 non-null  float64\n",
            " 16  income_ >50K      32561 non-null  float64\n",
            " 17  capital_gain_log  32561 non-null  float64\n",
            "dtypes: float64(5), int64(13)\n",
            "memory usage: 4.5 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pandas numpy scikit-learn ppscore -y\n",
        "!pip install pandas==1.5.3 numpy==1.23.5 scikit-learn==1.2.2\n",
        "!pip install -q ppscore\n",
        "\n",
        "# After running these commands, you might need to restart your kernel\n",
        "# to ensure the newly installed packages are loaded correctly.\n",
        "# Then, you can rerun your notebook cells from the beginning.\n",
        "\n",
        "# Re-import necessary libraries\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import ppscore as pps\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n--- Feature Selection ---\")\n",
        "\n",
        "# Use Isolation Forest to identify outliers\n",
        "print(\"\\nUsing Isolation Forest to identify outliers...\")\n",
        "# Initialize Isolation Forest. contamination can be an estimated proportion of outliers.\n",
        "# auto: decides the threshold based on the training data.\n",
        "# float: specifies the proportion of outliers in the data set.\n",
        "# An alternative is to set a fixed threshold on the anomaly scores.\n",
        "# Let's use 'auto' for simplicity initially.\n",
        "iso_forest = IsolationForest(contamination='auto', random_state=42)\n",
        "\n",
        "# Fit the model and predict outliers. -1 for outliers, 1 for inliers.\n",
        "# It's often recommended to fit Isolation Forest only on numerical features.\n",
        "# Ensure we use the numerical columns after all preprocessing steps.\n",
        "# Make sure df is loaded or available from previous cells.\n",
        "# If running this cell independently after restarting the kernel, you might need to load df again.\n",
        "# Example of reloading (adjust path as needed):\n",
        "# try:\n",
        "#     df = pd.read_csv('adult_with_headers.csv')\n",
        "#     # Re-run necessary preprocessing/feature engineering steps here if starting from scratch\n",
        "# except FileNotFoundError:\n",
        "#     print(\"Error: 'adult_with_headers.csv' not found. Please replace it with the correct file path.\")\n",
        "#     # Create a dummy DataFrame for demonstration if the file is not found\n",
        "#     data = {'col1': [1, 2, 3, 4, 5, None],\n",
        "#             'col2': [10, 20, None, 40, 50, 60],\n",
        "#             'col3': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
        "#             'col4': [100, 200, 300, 400, 500, 600]}\n",
        "#     df = pd.DataFrame(data)\n",
        "#     print(\"Using a dummy DataFrame for demonstration.\")\n",
        "#     # Manually apply preprocessing steps here if needed to match the state before the error\n",
        "#     # This is highly dependent on your previous steps.\n",
        "\n",
        "# The dataframe 'df' is expected to be available from the previous notebook cells.\n",
        "# If running this cell in isolation, you would need to load and preprocess the data first.\n",
        "\n",
        "numerical_cols_for_outliers = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Handle potential issue if the target variable is numerical and included here.\n",
        "# Assume 'income_>=50K' or similar might be the target if using adult dataset.\n",
        "# If you know your target column, exclude it. Example:\n",
        "target_col = 'income_>50K' # Replace with your actual target column name if numerical\n",
        "# Check if the potential target column exists and is numerical before attempting to remove it\n",
        "if target_col in numerical_cols_for_outliers:\n",
        "    numerical_cols_for_outliers.remove(target_col)\n",
        "    print(f\"Removed target column '{target_col}' from outlier detection features.\")\n",
        "\n",
        "if not numerical_cols_for_outliers:\n",
        "    print(\"No numerical features available for Isolation Forest after removing target/identifying numerical columns.\")\n",
        "    df_no_outliers = df.copy() # Proceed with original DataFrame if no numerical features\n",
        "else:\n",
        "    # Isolation Forest expects numpy array or similar, convert from DataFrame subset\n",
        "    X_numerical = df[numerical_cols_for_outliers].values\n",
        "\n",
        "    try:\n",
        "        outlier_predictions = iso_forest.fit_predict(X_numerical)\n",
        "\n",
        "        # Filter the DataFrame to keep only inliers (where prediction is 1)\n",
        "        df_no_outliers = df.iloc[outlier_predictions == 1].copy() # Use iloc to preserve original index alignment\n",
        "\n",
        "        num_outliers_detected = (outlier_predictions == -1).sum()\n",
        "        original_rows = len(df)\n",
        "        remaining_rows = len(df_no_outliers)\n",
        "\n",
        "        print(f\"Original number of rows: {original_rows}\")\n",
        "        print(f\"Number of outliers detected and removed: {num_outliers_detected}\")\n",
        "        print(f\"Number of rows remaining after outlier removal: {remaining_rows}\")\n",
        "        print(f\"Percentage of rows removed: {(num_outliers_detected / original_rows) * 100:.2f}%\")\n",
        "\n",
        "        print(\"\\nDataFrame head after outlier removal:\")\n",
        "        print(df_no_outliers.head())\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Isolation Forest outlier detection: {e}\")\n",
        "        print(\"Proceeding without removing outliers.\")\n",
        "        df_no_outliers = df.copy() # Revert to original DataFrame\n",
        "\n",
        "print(\"\\nDiscussion on how outliers can affect model performance:\")\n",
        "print(\"- Outliers are data points that are significantly different from other observations.\")\n",
        "print(\"- They can disproportionately influence the parameters of many machine learning algorithms, especially those based on distance metrics or assuming normality (e.g., Linear Regression, K-Means, PCA).\")\n",
        "print(\"- This influence can lead to skewed model estimates, wider confidence intervals, and ultimately, a model that performs poorly on new, unseen data.\")\n",
        "print(\"- Outliers can also affect scaling techniques (like Min-Max scaling) and correlation calculations.\")\n",
        "print(\"- Tree-based models (like Decision Trees, Random Forests, Gradient Boosting) are generally more robust to outliers because their splitting criteria are based on relative order of values rather than absolute magnitudes, although extreme outliers can still influence splits, especially early on.\")\n",
        "print(\"- Isolation Forest works by isolating observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Recursive partitioning results in shorter paths for outliers, which are fewer and farther apart, than for inliers.\")\n",
        "\n",
        "\n",
        "# Apply PPS (Predictive Power Score)\n",
        "print(\"\\nApplying PPS (Predictive Power Score)...\")\n",
        "# PPS works best on the DataFrame *after* outlier removal, if outliers were removed.\n",
        "# If outlier removal failed or wasn't performed, use the latest df.\n",
        "df_for_pps = df_no_outliers.copy()\n",
        "\n",
        "# PPS can take some time on large datasets, especially with many columns.\n",
        "# Calculate PPS matrix\n",
        "try:\n",
        "    # pps.matrix() calculates the PPS for all column pairs\n",
        "    pps_matrix = pps.matrix(df_for_pps)\n",
        "    print(\"\\nPPS Matrix (subset):\")\n",
        "    # Print a smaller subset or visualize the matrix for better readability\n",
        "    print(pps_matrix[['x', 'y', 'ppscore', 'kind']].head()) # Show a few rows\n",
        "\n",
        "    # Convert PPS matrix to a more convenient pivot table format\n",
        "    # pps_pivot = pps_matrix.pivot(index='x', columns='y', values='ppscore')\n",
        "    # print(\"\\nPPS Pivot Table (subset):\")\n",
        "    # print(pps_pivot.head()) # Show head of pivot\n",
        "\n",
        "    # Visualize the PPS matrix as a heatmap\n",
        "    # It's often better to visualize only the PPS between features and the target variable\n",
        "    # Let's assume the target is one of the columns.\n",
        "    # If your target is categorical and label encoded, PPS can still work.\n",
        "    # If your target was one-hot encoded into multiple columns, you might calculate PPS for each.\n",
        "    # Let's find the target column assuming it's the one not used for OneHotEncoding or LabelEncoding above\n",
        "    # A common target in the adult dataset is related to income, e.g., '>50K' vs '<=50K'\n",
        "\n",
        "    # Attempt to identify a potential target column based on previous steps\n",
        "    # This is a heuristic and might need manual adjustment based on your dataset\n",
        "    all_cols = set(df.columns)\n",
        "    # Note: Reading the CSV again here assumes the original file is still accessible.\n",
        "    # In a real scenario, you might have stored the original column names differently\n",
        "    # or know the target column explicitly.\n",
        "    try:\n",
        "        original_cols = set(pd.read_csv('adult_with_headers.csv').columns)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Warning: Could not read original CSV to identify potential target column based on original columns.\")\n",
        "        original_cols = set(df.columns) # Fallback to current columns\n",
        "\n",
        "    # Target column is likely one that was either LabelEncoded or is the original categorical target\n",
        "    # If the target was one-hot encoded, PPS will show relationships with each resulting binary column.\n",
        "\n",
        "    # Let's assume the target is 'income_>50K' or similar based on common adult datasets.\n",
        "    # If your target is different, replace 'income_>50K' below.\n",
        "    # Also consider if the target was label encoded, its name might remain the same.\n",
        "    # Check both original column names and encoded names.\n",
        "    # It's safer to explicitly define your target column name if known.\n",
        "    # For example, if the target is the original 'income' column which was LabelEncoded:\n",
        "    # target_col_actual = 'income'\n",
        "    # Or if it was One-Hot encoded:\n",
        "    # target_col_actual = 'income_>50K' # Assuming this is one of the resulting columns\n",
        "\n",
        "    # Based on the prompt (Feature Selection is before model training), the target might not be explicitly defined yet.\n",
        "    # However, PPS is often used relative to the target.\n",
        "    # Let's refine the potential target columns search to look for common patterns or the last column (common for targets).\n",
        "    potential_target_cols = [col for col in df_for_pps.columns if 'income_' in col or 'target' in col or 'salary' in col]\n",
        "    # Add the last column as a potential target if it's not already in the list\n",
        "    if df_for_pps.columns.tolist() and df_for_pps.columns[-1] not in potential_target_cols:\n",
        "         potential_target_cols.append(df_for_pps.columns[-1])\n",
        "\n",
        "    # Filter for columns that actually exist in the DataFrame\n",
        "    potential_target_cols = [col for col in potential_target_cols if col in df_for_pps.columns]\n",
        "\n",
        "    if potential_target_cols:\n",
        "        target_pps_matrix = pps_matrix[pps_matrix['y'].isin(potential_target_cols)]\n",
        "        print(f\"\\nPPS scores predicting potential target column(s): {potential_target_cols}\")\n",
        "\n",
        "        # Pivot for heatmap visualization\n",
        "        if not target_pps_matrix.empty:\n",
        "            # Ensure 'x' and 'y' columns exist and are used correctly for pivoting\n",
        "            if 'x' in target_pps_matrix.columns and 'y' in target_pps_matrix.columns:\n",
        "                # Drop rows with NaN in 'ppscore' or relevant columns before pivoting if necessary\n",
        "                 target_pps_matrix = target_pps_matrix.dropna(subset=['x', 'y', 'ppscore'])\n",
        "                 if not target_pps_matrix.empty:\n",
        "                    # Handle potential duplicate entries if pivot fails\n",
        "                    target_pps_pivot = target_pps_matrix.pivot_table(index='x', columns='y', values='ppscore', aggfunc='first')\n",
        "\n",
        "\n",
        "                    plt.figure(figsize=(8, max(6, len(target_pps_pivot.index) * 0.3))) # Adjust figure size based on number of features\n",
        "                    sns.heatmap(target_pps_pivot, annot=True, cmap='Blues', fmt=\".2f\")\n",
        "                    plt.title('PPS Predicting Potential Target Column(s)')\n",
        "                    plt.show()\n",
        "                 else:\n",
        "                    print(\"No valid data remaining for PPS heatmap after dropping NaNs.\")\n",
        "            else:\n",
        "                 print(\"PPS matrix does not contain 'x' or 'y' columns required for pivoting.\")\n",
        "        else:\n",
        "            print(\"No PPS relationships found for the identified potential target columns.\")\n",
        "\n",
        "        # Also visualize PPS predicting *other* features\n",
        "        # This shows if other features can predict each other, which can indicate multicollinearity\n",
        "        # Filter out rows where x predicts y, but not where y predicts x if you want unique pairs\n",
        "        # Or just show the full matrix heatmap (can be large)\n",
        "        # For simplicity, let's show PPS where ppscore is significant (>0.2, for example)\n",
        "        significant_pps = pps_matrix[(pps_matrix['ppscore'] > 0.2) & (pps_matrix['x'] != pps_matrix['y'])].sort_values(by='ppscore', ascending=False)\n",
        "        print(\"\\nSignificant PPS relationships (ppscore > 0.2, excluding self-prediction):\")\n",
        "        print(significant_pps[['x', 'y', 'ppscore', 'kind']].head(15)) # Show top 15 significant relationships\n",
        "\n",
        "    else:\n",
        "         print(\"\\nCould not identify any relevant potential target columns in the DataFrame for PPS visualization.\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while calculating or visualizing PPS: {e}\")\n",
        "    print(\"Please ensure 'ppscore' library is installed and your DataFrame is suitable.\")\n",
        "    pps_matrix = None # Ensure pps_matrix is None if calculation failed\n",
        "\n",
        "# Compare PPS findings with the correlation matrix\n",
        "print(\"\\nComparing PPS findings with the correlation matrix:\")\n",
        "\n",
        "# Calculate the correlation matrix using the DataFrame after outlier removal\n",
        "# Correlation works only for numerical features.\n",
        "df_numerical_for_corr = df_for_pps.select_dtypes(include=np.number)\n",
        "\n",
        "if not df_numerical_for_corr.empty:\n",
        "    correlation_matrix = df_numerical_for_corr.corr()\n",
        "\n",
        "    print(\"\\nCorrelation Matrix (absolute values):\")\n",
        "    # Visualize the correlation matrix as a heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix.abs(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "    plt.title('Absolute Correlation Matrix of Numerical Features (After Outlier Removal)')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nDiscussion comparing PPS and Correlation:\")\n",
        "    print(\"- **Correlation** measures the linear relationship between two numerical variables. It ranges from -1 (perfect negative linear correlation) to +1 (perfect positive linear correlation). A value near 0 indicates a weak linear relationship.\")\n",
        "    print(\"- **PPS** measures the predictive power of one column (predictor, x) predicting another column (target, y). It ranges from 0 (x has no predictive power for y) to 1 (x perfectly predicts y).\")\n",
        "    print(\"- **Key Differences:**\")\n",
        "    print(\"  - Correlation is **symmetric** (Corr(x, y) = Corr(y, x)), while PPS is **asymmetric** (PPS(x, y) != PPS(y, x) generally). PPS(x, y) tells you how well x predicts y, and PPS(y, x) tells you how well y predicts x. These can be very different.\")\n",
        "    print(\"  - Correlation only captures **linear** relationships between **numerical** variables.\")\n",
        "    print(\"  - PPS can capture **linear and non-linear** relationships and works with **numerical and categorical** variables.\")\n",
        "    print(\"  - PPS indicates the **strength of prediction**, which is often more relevant for feature selection than just linear correlation.\")\n",
        "    print(\"- **Insights from Comparison:**\")\n",
        "    print(\"  - A high correlation might imply a high PPS in both directions, but not always, especially if the relationship isn't purely linear.\")\n",
        "    print(\"  - PPS can reveal strong predictive relationships between features that have low or zero linear correlation (e.g., non-linear dependencies).\")\n",
        "    print(\"  - PPS is particularly useful for assessing the predictive power of features relative to the **target variable**, even if the target is categorical (if it's been encoded appropriately, or by using PPS functions that handle categorical targets).\")\n",
        "    print(\"  - Comparing Corr(x, y) and PPS(x, y) can highlight non-linear dependencies or the impact of outliers (though outliers were addressed before calculating here).\")\n",
        "    print(\"  - High PPS between two predictor features (x and y) indicates multicollinearity or redundancy in predictive power. This can help in deciding which features to keep or whether to use techniques like PCA or remove one of the features.\")\n",
        "\n",
        "    if pps_matrix is not None:\n",
        "        print(\"\\nExample comparison (pick a few pairs):\")\n",
        "        # Select a few pairs from the correlation matrix and compare with PPS\n",
        "        # Need to map back from potentially encoded columns if necessary.\n",
        "        # Let's look at pairs with high absolute correlation and compare their PPS.\n",
        "        high_corr_pairs = correlation_matrix.abs().stack().sort_values(ascending=False)\n",
        "        # Filter out self-correlation (corr=1) and duplicate pairs (corr(x,y) == corr(y,x))\n",
        "        # Check for empty Series before attempting to filter\n",
        "        if not high_corr_pairs.empty:\n",
        "            high_corr_pairs = high_corr_pairs[high_corr_pairs < 1]\n",
        "            high_corr_pairs = high_corr_pairs[~high_corr_pairs.index.duplicated(keep='first')] # Keep only one direction (e.g., (A,B) but not (B,A))\n",
        "\n",
        "            print(\"\\nTop 5 pairs by Absolute Correlation and their PPS:\")\n",
        "            for (col1, col2), corr_value in high_corr_pairs.head(5).items():\n",
        "                print(f\"Pair: ({col1}, {col2})\")\n",
        "                print(f\"  Absolute Correlation: {corr_value:.2f}\")\n",
        "                # Find PPS for both directions\n",
        "                # Safely access PPS score\n",
        "                pps_x_y_row = pps_matrix[(pps_matrix['x'] == col1) & (pps_matrix['y'] == col2)]\n",
        "                pps_x_y = pps_x_y_row['ppscore'].iloc[0] if not pps_x_y_row.empty else 'N/A'\n",
        "\n",
        "                pps_y_x_row = pps_matrix[(pps_matrix['x'] == col2) & (pps_matrix['y'] == col1)]\n",
        "                pps_y_x = pps_y_x_row['ppscore'].iloc[0] if not pps_y_x_row.empty else 'N/A'\n",
        "\n",
        "                print(f\"  PPS({col1} -> {col2}): {pps_x_y}\")\n",
        "                print(f\"  PPS({col2} -> {col1}): {pps_y_x}\")\n",
        "        else:\n",
        "            print(\"No high correlation pairs found in the numerical data.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo numerical columns available for correlation analysis.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- End of Feature Selection ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1477
        },
        "id": "DlnNLGhbs0GH",
        "outputId": "92ccc530-70e2-4c2b-9e1e-b2d7835d8e37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pandas 1.5.3\n",
            "Uninstalling pandas-1.5.3:\n",
            "  Successfully uninstalled pandas-1.5.3\n",
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: scikit-learn 1.6.1\n",
            "Uninstalling scikit-learn-1.6.1:\n",
            "  Successfully uninstalled scikit-learn-1.6.1\n",
            "Found existing installation: ppscore 1.3.0\n",
            "Uninstalling ppscore-1.3.0:\n",
            "  Successfully uninstalled ppscore-1.3.0\n",
            "Collecting pandas==1.5.3\n",
            "  Using cached pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting scikit-learn==1.2.2\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
            "Using cached pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pandas, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "bigframes 2.5.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.4 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5 pandas-1.5.3 scikit-learn-1.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f71effcd43284d83bd63ab596de42d00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2300139526>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Re-import necessary libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIsolationForest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mppscore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_output\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SetOutputMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m from .utils._tags import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/utils/murmurhash.pyx\u001b[0m in \u001b[0;36minit sklearn.utils.murmurhash\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1089
        },
        "id": "S9yxl0wwu_f8",
        "outputId": "bfa9003e-5fac-4aa1-db53-f3a3f112358b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Collecting scipy>=1.8.0 (from scikit-learn)\n",
            "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.1\n",
            "    Uninstalling joblib-1.5.1:\n",
            "      Successfully uninstalled joblib-1.5.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.0 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.5.1 numpy-2.3.0 scikit-learn-1.7.0 scipy-1.15.3 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "ff084a9d5be84cf992dbff39c7ea2065"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZrJZT_pwSH9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}